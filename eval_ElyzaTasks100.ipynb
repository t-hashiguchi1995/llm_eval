{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cookbook.openai.com/examples/gpt4o/introduction_to_gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./api\")\n",
    "\n",
    "import generate_openai\n",
    "import generate_claude\n",
    "import generate_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"elyza/ELYZA-tasks-100\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"input\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=dataset[\"input\"][0])\n",
    "openai_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_from_complettion_json(response_json):\n",
    "    return response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "openai_result = get_message_from_complettion_json(openai_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = generate_local_llm.load_model(\"elyza/ELYZA-japanese-Llama-2-7b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset[\"input\"][0]\n",
    "prompt = generate_local_llm.build_llama_prompt(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm_result = generate_local_llm.generate(model, tokenizer, prompt=prompt, max_new_tokens=MAX_NEW_TOKEN)\n",
    "local_llm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_claude.format_claude_prompt(dataset[\"input\"][0])\n",
    "claude_result = generate_claude.chat_complettion(model=\"anthropic.claude-v2:1\", prompt=prompt, max_tokens_to_sample=MAX_NEW_TOKEN)\n",
    "claude_result = claude_result.get('completion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4Judgeを入れる\n",
    "判定基準：https://huggingface.co/datasets/elyza/ELYZA-tasks-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_judge_prompt_ans = \"\"\"\n",
    "[指示]\n",
    "公平な判断者として行動し、以下に表示されるユーザーの質問に対するAIアシスタントの応答の品質を評価してください。\n",
    "あなたの評価は正確さと有用性を考慮すべきです。AIアシスタントの返答の言語は、ユーザーが使用している言語と一致しているべきで、そうでない場合は減点されるべきです。\n",
    "参照答えと判定基準とアシスタントの答えが与えられます。あなたの評価は、アシスタントの答えと参照答えを比較することから始めてください。\n",
    "比較後、あなたの評価は、アシスタントの答えを判定基準に従って評価してください。判定基準で判定できない場合、タスク評価基準に従って評価してください。\n",
    "ミスを特定し、訂正してください。できるだけ客観的であること。\n",
    "説明を提供した後、このフォーマットに厳密に従って1から10までのスケールで応答を評価する必要があります：\\\"[[評価]]\\\"、例えば：\\\"評価：[[5]]\\\"。\n",
    "\n",
    "[質問]\n",
    "{question}\n",
    "\n",
    "[参考回答の開始]\n",
    "{ref_answer_1}\n",
    "[参考回答の終了]\n",
    "\n",
    "[判定基準の開始]\n",
    "{eval_aspect}\n",
    "[判定基準の終了]\n",
    "\n",
    "[タスク評価基準の開始]\n",
    "{tasks_aspect}\n",
    "[タスク評価基準の終了]\n",
    "\n",
    "[アシスタントの回答の開始]\n",
    "{answer}\n",
    "[アシスタントの回答の終了]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elyza_tasks_aspect = \"\"\"\n",
    "[評価基準]\n",
    "点数についての基本的な評価基準は、以下のようになります。\n",
    "\n",
    "[基本的な評価基準]\n",
    "ベースとなる得点:\n",
    "- 1点: 誤っている\n",
    " - 指示に従えていない場合\n",
    " - 選択肢を選ぶ問題で、誤った選択肢を選んでいる場合\n",
    " - 筆記問題で全体的に事実と異なる内容を述べている場合\n",
    " - 部分的に事実と異なる内容を述べている場合は、減点項目にあるように部分的な誤りとして-1点の減点のみになります。\n",
    "- 2点: 誤っているが、方向性は合っている\n",
    " - 基本的に2点をつけることは少なく、3点から1点減点した場合に2点になる場合が多いです\n",
    " - 複数の問題や指示が与えられ、ほとんど間違っているがわずかに（1/3以下）正解している場合\n",
    " - 1つの入力に4問の指示が与えられ、そのうちの1つのみが正解している場合など\n",
    " - 後述の、過度に安全性を気にしていて答えられていない場合\n",
    "- 3点: 部分的に誤っている, 部分的に合っている\n",
    " - 複雑な指示が与えられ、そのうちの一部（半分以上）のみに従えている場合\n",
    " - フレンドリーなスタイルで要約してという指示に対し、要約はできているがフレンドリーな文体ではない場合など\n",
    " - 複数の問題が与えられ、そのうちの一部（半分以上）のみが正解している場合\n",
    " - 1つの入力に4問の指示が与えられ、そのうちの2~3つ正解している場合など\n",
    "- 4点: 合っている\n",
    " - 問いに対して正解している場合\n",
    " - 正解しているが、答えのみのぶっきらぼうな回答であるが場合や、当てずっぽうで選択肢を当てている場合\n",
    "- 5点: 役に立つ\n",
    " - 問いや指示に対して正しい回答をし、その上でユーザーが何に困っているかを想像し、役に立とうとしている場合\n",
    " - 例: 生徒の要約を改善するタスクで、ただ改善した要約を提示するだけでなく、どのような点を改善すればいいのかを説明している場合\n",
    "- 問いに対して正解していて、その上で理由や説明を提示していて役に立つ場合\n",
    " - ブレインストームや執筆系のタスクにおいて、想像力豊かな回答をしている場合\n",
    "\n",
    " [基本的な減点項目]\n",
    "ベースとなる得点から、以下のような要素を考慮して、得点を調整します。\n",
    "\n",
    "- 不自然な日本語: -1点\n",
    " - 一見して意味がわからない不自然な日本語、同じ文章の繰り返し、唐突に現れる英語\n",
    "- 部分的な誤り: -1点\n",
    " - 筆記問題で部分的に事実と異なる内容を述べている場合\n",
    "  - 全体的に事実と異なる内容を述べている場合、前述のベースとなる得点が1点になります（-4点扱い）\n",
    "  - 一見して事実か判断がつかない場合は、Googleなどで検索してチェックをしてください。\n",
    "  - ただし全てのファクトチェックを行うと作業が終わらないので、特に怪しい箇所などをランダムにピックアップして調べるのでもOKです。\n",
    "- 過度な安全性: 2点にする\n",
    " - 「倫理的に答えられません」というような回答\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"output\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"eval_aspect\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "openai_eval = gpt4_judge_prompt_ans.format(question=dataset[\"input\"][ind], ref_answer_1=dataset[\"output\"][ind], eval_aspect=dataset[\"eval_aspect\"][ind], tasks_aspect=elyza_tasks_aspect, answer=openai_result)\n",
    "local_llm_eval = gpt4_judge_prompt_ans.format(question=dataset[\"input\"][ind], ref_answer_1=dataset[\"output\"][ind], eval_aspect=dataset[\"eval_aspect\"][ind], tasks_aspect=elyza_tasks_aspect, answer=local_llm_result)\n",
    "claude_eval = gpt4_judge_prompt_ans.format(question=dataset[\"input\"][ind], ref_answer_1=dataset[\"output\"][ind], eval_aspect=dataset[\"eval_aspect\"][ind], tasks_aspect=elyza_tasks_aspect, answer=claude_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_eval_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=openai_eval)\n",
    "local_llm_eval_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=local_llm_eval)\n",
    "claude_eval_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=claude_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_eval_result = get_message_from_complettion_json(openai_eval_result)\n",
    "local_llm_eval_result = get_message_from_complettion_json(local_llm_eval_result)\n",
    "claude_eval_result = get_message_from_complettion_json(claude_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ElyzaTasks100/judge_gpt-4-turbo/gpt-4-turbo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df[\"score\"] = df[\"ELYZA-tasks-100評価結果\"].apply(lambda x: re.search(\"\\[\\[(.*)\\]\\]\", x).group(0))\n",
    "df[\"score\"] = df[\"score\"].str.replace(\"[\", \"\").str.replace(\"]\", \"\")\n",
    "df[\"score\"].astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
