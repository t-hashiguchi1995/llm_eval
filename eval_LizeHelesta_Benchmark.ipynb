{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./api\")\n",
    "\n",
    "import generate_openai\n",
    "import generate_claude\n",
    "import generate_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"thashiguchi/LizeHelesta_Benchmark\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(data):\n",
    "    result = f\"\"\"\n",
    "以下の4択クイズを回答せよ。必ず回答を選択すること。\n",
    "## 問題文\n",
    "{data['問題文']}\n",
    "\n",
    "## 選択肢\n",
    "- 選択肢1: {data['選択肢①']}\n",
    "- 選択肢2: {data['選択肢②']}\n",
    "- 選択肢3: {data['選択肢③']}\n",
    "- 選択肢4: {data['選択肢④']}\n",
    "    \"\"\"\n",
    "    return result\n",
    "\n",
    "def add_input_output(example):\n",
    "    example[\"input\"] = create_input(example)\n",
    "    example[\"output\"] = example[\"答え(正答率)\"]    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_input_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"input\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_judge_prompt_ans = \"\"\"\n",
    "[指示]\n",
    "公平な判断者として行動し、以下に表示される4択クイズに対するAIアシスタントのクイズの回答を評価してください。\n",
    "あなたの評価は正確さと正答率を考慮すべきです。正答率の高いクイズを誤っていたら、より減点されるべきです。\n",
    "AIアシスタントの返答の言語は、ユーザーが使用している言語と一致しているべきで、そうでない場合は減点されるべきです。\n",
    "正解とアシスタントの答えが与えられます。あなたの評価は、アシスタントの答えと正解を比較してください。\n",
    "正解について説明を提供した後、このフォーマットに厳密に従って1から10までのスケールで応答を評価する必要があります：\\\"[[評価]]\\\"、例えば：\\\"評価：[[5]]\\\"。\n",
    "\n",
    "[質問]\n",
    "{question}\n",
    "\n",
    "[正解の開始]\n",
    "{ref_answer_1}\n",
    "[正解の終了]\n",
    "\n",
    "[アシスタントの回答の開始]\n",
    "{answer}\n",
    "[アシスタントの回答の終了]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_from_complettion_json(response_json):\n",
    "    return response_json[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vtuberについて質問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Vtuberについて教えてください\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"gpt-4o\"\n",
    "openai_result = generate_openai.chat_complettion(model=openai_model, user_prompt=user_prompt)\n",
    "openai_result = get_message_from_complettion_json(openai_result)\n",
    "print(f\"******{openai_model}******\")\n",
    "print(openai_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "prompt = generate_claude.format_claude_v3_prompt(user_prompt, MAX_NEW_TOKEN)\n",
    "claude_result = generate_claude.claude_v3_chat_complettion(model=claude_model, prompt=prompt)\n",
    "claude_result = claude_result[\"content\"][0][\"text\"]\n",
    "print(f\"******{claude_model}******\")\n",
    "print(claude_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = \"gemini-1.5-flash-latest\"\n",
    "model, chat = generate_gemini.configure_model(gemini_model)\n",
    "gemini_response = generate_gemini.generate_content(model=model, prompt=user_prompt)\n",
    "print(f\"******{gemini_model}******\")\n",
    "print(gemini_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"elyza/Llama-3-ELYZA-JP-8B\"\n",
    "model, tokenizer = generate_local_llm.load_model(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_local_llm.build_llama_prompt(user_prompt, tokenizer)\n",
    "result = generate_local_llm.generate(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リゼヘルエスタについて質問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"リゼ・ヘルエスタについて教えてください\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = \"gpt-4o\"\n",
    "openai_result = generate_openai.chat_complettion(model=openai_model, user_prompt=user_prompt)\n",
    "openai_result = get_message_from_complettion_json(openai_result)\n",
    "print(f\"******{openai_model}******\")\n",
    "print(openai_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "prompt = generate_claude.format_claude_v3_prompt(user_prompt, MAX_NEW_TOKEN)\n",
    "claude_result = generate_claude.claude_v3_chat_complettion(model=claude_model, prompt=prompt)\n",
    "claude_result = claude_result[\"content\"][0][\"text\"]\n",
    "print(f\"******{claude_model}******\")\n",
    "print(claude_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = \"gemini-1.5-flash-latest\"\n",
    "model, chat = generate_gemini.configure_model(gemini_model)\n",
    "gemini_response = generate_gemini.generate_content(model=model, prompt=user_prompt)\n",
    "print(f\"******{gemini_model}******\")\n",
    "print(gemini_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_local_llm.build_llama_prompt(user_prompt, tokenizer)\n",
    "result = generate_local_llm.generate(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クイズの正答率を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "results = []\n",
    "for data in dataset:\n",
    "    user_prompt = data[\"input\"]\n",
    "    ref_answer_1 = dataset[\"output\"]\n",
    "    print(data[\"問題文\"])\n",
    "    for openai_model in [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]:\n",
    "        openai_result = generate_openai.chat_complettion(model=openai_model, user_prompt=user_prompt)\n",
    "        openai_result = get_message_from_complettion_json(openai_result)\n",
    "        openai_eval = gpt4_judge_prompt_ans.format(question=user_prompt, ref_answer_1=ref_answer_1, answer=openai_result)\n",
    "        openai_eval_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=openai_eval)\n",
    "        openai_eval_result = get_message_from_complettion_json(openai_eval_result)\n",
    "        results.append([user_prompt, openai_result, openai_eval_result, openai_model])\n",
    "        time.sleep(1)\n",
    "\n",
    "import pandas as pd\n",
    "df_ = pd.DataFrame(results)\n",
    "df_.columns = [\"input\", \"output\", \"eval\", \"model\"]\n",
    "df_.to_csv(\"LizeHelesta_Benchmark/openai_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "results = []\n",
    "for data in dataset:\n",
    "    user_prompt = data[\"input\"]\n",
    "    ref_answer_1 = dataset[\"output\"]\n",
    "    print(data[\"問題文\"])\n",
    "    for claude_model in [\"anthropic.claude-v2:1\", \"anthropic.claude-3-haiku-20240307-v1:0\", \"anthropic.claude-3-sonnet-20240229-v1:0\"]:\n",
    "        if claude_model == \"anthropic.claude-v2:1\":\n",
    "            prompt = generate_claude.format_claude_v2_prompt(user_prompt)\n",
    "            claude_result = generate_claude.claude_v2_chat_complettion(model=claude_model, prompt=prompt, max_tokens_to_sample=MAX_NEW_TOKEN)\n",
    "            claude_result = claude_result.get('completion')\n",
    "        else:\n",
    "            prompt = generate_claude.format_claude_v3_prompt(user_prompt, MAX_NEW_TOKEN)\n",
    "            claude_result = generate_claude.claude_v3_chat_complettion(model=claude_model, prompt=prompt)\n",
    "            claude_result = claude_result[\"content\"][0][\"text\"]\n",
    "        claude_eval = gpt4_judge_prompt_ans.format(question=user_prompt, ref_answer_1=ref_answer_1, answer=claude_result)\n",
    "        claude_eval_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=claude_eval)\n",
    "        claude_eval_result = get_message_from_complettion_json(claude_eval_result)\n",
    "        results.append([user_prompt, claude_result, claude_eval_result, claude_model])\n",
    "        time.sleep(1)\n",
    "\n",
    "import pandas as pd\n",
    "df_ = pd.DataFrame(results)\n",
    "df_.columns = [\"input\", \"output\", \"eval\", \"model\"]\n",
    "df_.to_csv(\"LizeHelesta_Benchmark/claude_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "results = []\n",
    "\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    }\n",
    "]\n",
    "for data in dataset:\n",
    "    user_prompt = data[\"input\"]\n",
    "    ref_answer_1 = dataset[\"output\"]\n",
    "    print(data[\"問題文\"])\n",
    "    for gemini_model in [\"gemini-1.5-flash-latest\", \"gemini-1.5-pro-latest\"]:\n",
    "        model, chat = generate_gemini.configure_model(gemini_model)\n",
    "        gemini_response = generate_gemini.generate_content(model=model, prompt=user_prompt, safety_settings=safety_settings)\n",
    "        gemini_result = gemini_response.text\n",
    "        gemini_eval = gpt4_judge_prompt_ans.format(question=user_prompt, ref_answer_1=ref_answer_1, answer=gemini_result)\n",
    "        gemini_eval_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=gemini_eval)\n",
    "        gemini_eval_result = get_message_from_complettion_json(gemini_eval_result)\n",
    "        results.append([user_prompt, gemini_result, gemini_eval_result, gemini_model])\n",
    "        time.sleep(1)\n",
    "\n",
    "import pandas as pd\n",
    "df_ = pd.DataFrame(results)\n",
    "df_.columns = [\"input\", \"output\", \"eval\", \"model\"]\n",
    "df_.to_csv(\"LizeHelesta_Benchmark/gemini_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://host.docker.internal:8881')\n",
    "\n",
    "def ollama_chat(model_name, user_prompt):\n",
    "    response = client.chat(model=model_name, messages=[\n",
    "        {\n",
    "        'role': 'system',\n",
    "        'content': \"あなたはAIアシスタントです。聞かれた内容について誠実に回答してください。\"\n",
    "        },\n",
    "        {\n",
    "        'role': 'user',\n",
    "        'content': user_prompt\n",
    "        },\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_models = []\n",
    "for model in client.list()[\"models\"]:\n",
    "    ollama_models.append(model[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in ollama_models:\n",
    "    print(model)\n",
    "    for data in dataset:\n",
    "        user_prompt = data[\"input\"]\n",
    "        ref_answer_1 = dataset[\"output\"]\n",
    "        local_result = ollama_chat(model, user_prompt)\n",
    "        local_eval = gpt4_judge_prompt_ans.format(question=user_prompt, ref_answer_1=ref_answer_1, answer=local_result)\n",
    "        local_eval_result = generate_openai.chat_complettion(model=\"gpt-4o\", user_prompt=local_eval)\n",
    "        local_eval_result = get_message_from_complettion_json(local_eval_result)\n",
    "        results.append([user_prompt, local_result, local_eval_result, model])\n",
    "import pandas as pd\n",
    "df_ = pd.DataFrame(results)\n",
    "df_.columns = [\"input\", \"output\", \"eval\", \"model\"]\n",
    "df_.to_csv(\"LizeHelesta_Benchmark/local_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir(\"./LizeHelesta_Benchmark\"):\n",
    "    _df = pd.read_csv(f\"./LizeHelesta_Benchmark/{file}\")\n",
    "    df = pd.concat([df, _df])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(row):\n",
    "    pattern = r'\\[(\\d{1,2})\\]'\n",
    "    text = row[\"eval\"]\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        \n",
    "        pattern = \"評価\\].*(\\d{1,2})\"\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"] = df.apply(lambda x: scoring(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"] = df[\"score\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"model\", \"score\"]].groupby(\"model\").mean().sort_values(\"score\", ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
